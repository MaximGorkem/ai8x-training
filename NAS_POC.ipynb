{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 63,
   "metadata": {},
   "outputs": [],
   "source": [
    "### MOFA: Maxim's Once For All ###\n",
    "### This version is using the Kernel Transition Matrix by default. ###"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "metadata": {},
   "outputs": [],
   "source": [
    "import copy\n",
    "import random\n",
    "import os\n",
    "import time\n",
    "\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "from torch.utils.data import DataLoader\n",
    "from torch.optim.lr_scheduler import StepLR\n",
    "import torchvision\n",
    "from torchvision import transforms\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "from datasets.cifar100 import cifar100_get_datasets\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# <ins>Classes</ins>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Clamp(nn.Module):\n",
    "    \"\"\"\n",
    "    Post-Activation Clamping Module\n",
    "    Clamp the output to the given range (typically, [-128, +127])\n",
    "    \"\"\"\n",
    "    def __init__(self, min_val=None, max_val=None):\n",
    "        super().__init__()\n",
    "        self.min_val = min_val\n",
    "        self.max_val = max_val\n",
    "\n",
    "    def forward(self, x):  # pylint: disable=arguments-differ\n",
    "        \"\"\"Forward prop\"\"\"\n",
    "        return x.clamp(min=self.min_val, max=self.max_val)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "class MOFAnet(nn.Module):\n",
    "    # Maxim OFA Net\n",
    "    def __init__(self, param_dict):\n",
    "        super(MOFAnet, self).__init__()\n",
    "        self.param_dict = param_dict\n",
    "        self.in_ch = param_dict['in_ch']\n",
    "        self.out_class = param_dict['out_class']\n",
    "        self.n_units = param_dict['n_units']\n",
    "        self.width_list = param_dict['width_list']\n",
    "        self.kernel_list = param_dict['kernel_list']\n",
    "        self.bias_list = param_dict['bias_list']\n",
    "        self.bn = param_dict['bn']\n",
    "        self.last_width = self.in_ch\n",
    "        self.units = nn.ModuleList([])\n",
    "        for i in range(n_units):\n",
    "            self.units.append(Unit(len(self.kernel_list[i]), \n",
    "                                   self.kernel_list[i],\n",
    "                                   self.width_list[i], \n",
    "                                   self.last_width, \n",
    "                                   self.bias_list[i],\n",
    "                                   self.bn))\n",
    "            self.last_width = self.width_list[i][-1]\n",
    "        self.flatten = nn.Flatten()\n",
    "        self.max_pool = nn.MaxPool2d(kernel_size=2)\n",
    "\n",
    "        self.classifier = nn.Linear(1024, self.out_class) \n",
    "    def forward(self, x):\n",
    "        for i, unit in enumerate(self.units[:-1]):\n",
    "            x = unit(x)\n",
    "            x = self.max_pool(x)\n",
    "        x = self.units[-1](x)\n",
    "        x = x.view(x.size(0), -1)\n",
    "        x = self.classifier(x)\n",
    "        return x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Unit(nn.Module):\n",
    "    def __init__(self, depth, kernel_list, \n",
    "                 width_list, init_width, bias_list, bn=True):\n",
    "        super(Unit, self).__init__()\n",
    "        self.depth = depth\n",
    "        self.kernel_list = kernel_list\n",
    "        self.width_list = width_list\n",
    "        self.bias_list = bias_list\n",
    "        self.bn = bn\n",
    "        self._width_list = [init_width] + width_list\n",
    "        self.layers = nn.ModuleList([])\n",
    "        for i in range(depth):\n",
    "            self.layers.append(\n",
    "                FusedConv2dReLU(self._width_list[i],\n",
    "                                self._width_list[i+1],\n",
    "                                self.kernel_list[i],\n",
    "                                self.bias_list[i],\n",
    "                                self.bn))\n",
    "    def forward(self, x):\n",
    "        for i in range(self.depth):\n",
    "            x = self.layers[i](x)\n",
    "        return x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "class FusedConv2dReLU(nn.Module):\n",
    "    def __init__(\n",
    "            self,\n",
    "            in_channels,\n",
    "            out_channels,\n",
    "            kernel_size,\n",
    "            bias=True,\n",
    "            bn=True):\n",
    "        super(FusedConv2dReLU, self).__init__()\n",
    "        self.in_channels = in_channels\n",
    "        self.out_channels = out_channels\n",
    "        self.kernel_size = kernel_size\n",
    "            \n",
    "        ktm_core = torch.zeros((9, 1))\n",
    "        ktm_core[4] = 1\n",
    "        self.ktm = nn.Parameter(data=ktm_core, requires_grad=True)\n",
    "        \n",
    "        if kernel_size == 1:\n",
    "            self.pad = 0\n",
    "        elif kernel_size == 3:\n",
    "            self.pad = 1\n",
    "        else:\n",
    "            raise ValueError\n",
    "        self.func = F.conv2d\n",
    "        self.conv2d = nn.Conv2d(in_channels, out_channels,\n",
    "                                kernel_size=3, stride=1,\n",
    "                                padding=1, bias=bias)\n",
    "        self.bn = bn\n",
    "        if self.bn:\n",
    "            self.batchnorm = nn.BatchNorm2d(out_channels)\n",
    "        self.activation = nn.ReLU()\n",
    "        self.clamp = Clamp(min_val=-1, max_val=1)\n",
    "    def forward(self, x):        \n",
    "        weight = self.conv2d.weight\n",
    "        bias = self.conv2d.bias\n",
    "        if self.kernel_size == 1:\n",
    "            flattened_weight = weight.view(weight.size(0), weight.size(1), -1, 9)\n",
    "            weight = flattened_weight.to(device) @ self.ktm.to(device)\n",
    "                    \n",
    "        x = self.func(x, weight, bias, self.conv2d.stride, self.pad)\n",
    "        if self.bn:\n",
    "            x = self.batchnorm(x)\n",
    "        x = self.activation(x)\n",
    "#         x = self.clamp(x)\n",
    "        return x"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# <ins>Functions</ins>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Batchnorm Related "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "def make_bn_stats_false(model):\n",
    "    for u_ind, unit in enumerate(model.units):\n",
    "        for l_ind, layer in enumerate(unit.layers):\n",
    "            model.units[u_ind].layers[l_ind].batchnorm.track_running_stats = False\n",
    "\n",
    "    return model\n",
    "\n",
    "def make_bn_stats_true(model):\n",
    "    for u_ind, unit in enumerate(model.units):\n",
    "        for l_ind, layer in enumerate(unit.layers):\n",
    "            model.units[u_ind].layers[l_ind].batchnorm.track_running_stats = True\n",
    "\n",
    "    return model\n",
    "\n",
    "def fuse_bn(conv, bn):\n",
    "    w = conv.weight\n",
    "    mean = bn.running_mean\n",
    "    var_sqrt = torch.sqrt(bn.running_var + bn.eps)\n",
    "    beta = bn.weight\n",
    "    gamma = bn.bias\n",
    "    if conv.bias is not None:\n",
    "        b = conv.bias\n",
    "    else:\n",
    "        b = mean.new_zeros(mean.shape)\n",
    "    w = w * (beta / var_sqrt).reshape([conv.out_channels, 1, 1, 1])\n",
    "    b = (b - mean) / var_sqrt * beta + gamma\n",
    "    fused_conv = nn.Conv2d(conv.in_channels,\n",
    "                         conv.out_channels,\n",
    "                         conv.kernel_size,\n",
    "                         conv.stride,\n",
    "                         conv.padding,\n",
    "                         bias=True)\n",
    "    fused_conv.weight = nn.Parameter(w)\n",
    "    fused_conv.bias = nn.Parameter(b)\n",
    "    return fused_conv\n",
    "\n",
    "\n",
    "def fuse_bn_mofa(mofa_net):\n",
    "    param_dict = copy.deepcopy(mofa_net.param_dict)\n",
    "    param_dict['bn'] = False\n",
    "    fused_model = MOFAnet(param_dict)\n",
    "    with torch.no_grad():\n",
    "        fused_model.classifier.weight.copy_(mofa_net.classifier.weight)\n",
    "        fused_model.classifier.bias.copy_(mofa_net.classifier.bias)\n",
    "    for u_ind, unit in enumerate(mofa_net.units):\n",
    "        for l_ind, layer in enumerate(unit.layers):\n",
    "            fused_conv = fuse_bn(layer.conv2d, layer.batchnorm)\n",
    "            fused_conv = fused_conv.to(device)\n",
    "            with torch.no_grad():\n",
    "                fused_model.units[u_ind].layers[l_ind].conv2d.weight.copy_(fused_conv.weight)\n",
    "                fused_model.units[u_ind].layers[l_ind].conv2d.bias.copy_(fused_conv.bias)\n",
    "    return fused_model"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Elastic Kernel - Depth -Width "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def sample_subnet_kernel(mofa):\n",
    "    param_dict = copy.deepcopy(mofa.param_dict)\n",
    "    for u_ind, unit in enumerate(mofa.units):\n",
    "        for l_ind, layer in enumerate(unit.layers):\n",
    "            param_dict['kernel_list'][u_ind][l_ind] = random.choice([1, 3])\n",
    "#     param_dict['kernel_list'][0][0] = random.choice([1, 3])\n",
    "#     param_dict['kernel_list'][0][1] = random.choice([1, 3])\n",
    "#     param_dict['kernel_list'][0][2] = random.choice([1, 3])\n",
    "    param_dict['bn'] = False\n",
    "    subnet = MOFAnet(param_dict)\n",
    "    with torch.no_grad():\n",
    "        subnet.classifier.weight.copy_(mofa.classifier.weight)\n",
    "        subnet.classifier.bias.copy_(mofa.classifier.bias)\n",
    "        for u_ind, unit in enumerate(mofa.units):\n",
    "            for l_ind, layer in enumerate(unit.layers):\n",
    "                subnet.units[u_ind].layers[l_ind].conv2d.weight.copy_(mofa.units[u_ind].layers[l_ind].conv2d.weight)\n",
    "                subnet.units[u_ind].layers[l_ind].ktm.copy_(mofa.units[u_ind].layers[l_ind].ktm)\n",
    "                if mofa.bias_list[u_ind][l_ind] is True:\n",
    "                    subnet.units[u_ind].layers[l_ind].conv2d.bias.copy_(mofa.units[u_ind].layers[l_ind].conv2d.bias)\n",
    "    return subnet\n",
    "\n",
    "def update_mofa_from_subnet_kernel(mofa, subnet):\n",
    "    with torch.no_grad():\n",
    "        mofa.classifier.weight.copy_(subnet.classifier.weight)\n",
    "        mofa.classifier.bias.copy_(subnet.classifier.bias)\n",
    "        for u_ind, unit in enumerate(mofa.units):\n",
    "            for l_ind, layer in enumerate(unit.layers):\n",
    "                mofa.units[u_ind].layers[l_ind].conv2d.weight.copy_(subnet.units[u_ind].layers[l_ind].conv2d.weight)\n",
    "                mofa.units[u_ind].layers[l_ind].ktm.copy_(subnet.units[u_ind].layers[l_ind].ktm)\n",
    "                if mofa.bias_list[u_ind][l_ind] is True:\n",
    "                    mofa.units[u_ind].layers[l_ind].conv2d.bias.copy_(subnet.units[u_ind].layers[l_ind].conv2d.bias)\n",
    "    return mofa\n",
    "\n",
    "\n",
    "def sample_subnet_depth(mofa, sample_kernel=True):\n",
    "    param_dict = copy.deepcopy(mofa.param_dict)\n",
    "    depth_list = []\n",
    "    for u_ind in range(len(param_dict['width_list'])):\n",
    "        max_depth = len(param_dict['width_list'][u_ind])\n",
    "        min_depth = 1\n",
    "        depth_list.append(random.randint(min_depth, max_depth))\n",
    "    \n",
    "    if sample_kernel:\n",
    "        subnet = sample_subnet_kernel(mofa) # This is confirmed by Ji\n",
    "    else:\n",
    "        subnet = copy.deepcopy(mofa)\n",
    "    \n",
    "    param_dict = copy.deepcopy(subnet.param_dict)\n",
    "    param_dict['bn'] = False\n",
    "    param_dict['width_list'] = [lst[:depth_list[ind]] for ind, lst in enumerate(subnet.param_dict['width_list'])]\n",
    "    param_dict['kernel_list'] = [lst[:depth_list[ind]] for ind, lst in enumerate(subnet.param_dict['kernel_list'])]\n",
    "    param_dict['bias_list'] = [lst[:depth_list[ind]] for ind, lst in enumerate(subnet.param_dict['bias_list'])]\n",
    "    \n",
    "    subnet2 = MOFAnet(param_dict)\n",
    "    \n",
    "    with torch.no_grad():\n",
    "        subnet2.classifier.weight.copy_(subnet.classifier.weight)\n",
    "        subnet2.classifier.bias.copy_(subnet.classifier.bias)\n",
    "        for u_ind, unit in enumerate(subnet2.units):\n",
    "            for l_ind, layer in enumerate(unit.layers):\n",
    "                subnet2.units[u_ind].layers[l_ind].conv2d.weight.copy_(subnet.units[u_ind].layers[l_ind].conv2d.weight)\n",
    "                subnet2.units[u_ind].layers[l_ind].conv2d.bias.copy_(subnet.units[u_ind].layers[l_ind].conv2d.bias)\n",
    "        \n",
    "    return subnet2, param_dict, depth_list\n",
    "\n",
    "def update_mofa_from_subnet_depth(mofa, subnet):\n",
    "    subnet_params = subnet.param_dict\n",
    "    mofa_params = mofa.param_dict\n",
    "    \n",
    "    with torch.no_grad():\n",
    "        mofa.classifier.weight.copy_(subnet.classifier.weight)\n",
    "        mofa.classifier.bias.copy_(subnet.classifier.bias)\n",
    "        for u_ind, unit in enumerate(subnet.units):\n",
    "                for l_ind, layer in enumerate(unit.layers):\n",
    "                    mofa.units[u_ind].layers[l_ind].conv2d.weight.copy_(subnet.units[u_ind].layers[l_ind].conv2d.weight)\n",
    "                    mofa.units[u_ind].layers[l_ind].ktm.copy_(subnet.units[u_ind].layers[l_ind].ktm)\n",
    "                    if mofa.bias_list[u_ind][l_ind] is True:\n",
    "                        mofa.units[u_ind].layers[l_ind].conv2d.bias.copy_(subnet.units[u_ind].layers[l_ind].conv2d.bias)\n",
    "    return mofa\n",
    "                \n",
    "\n",
    "# def sample_subnet_width(mofa):\n",
    " #TODO\n",
    "    \n",
    "# def update_mofa_from_subnet_width(mofa, subnet):\n",
    " #TODO"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Others"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "metadata": {},
   "outputs": [],
   "source": [
    "def cross_entropy_loss_with_soft_target(pred, soft_target):\n",
    "    logsoftmax = nn.LogSoftmax()\n",
    "    return torch.mean(torch.sum(- soft_target * logsoftmax(pred), 1))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# <ins> MOFA Training </ins>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "n_units = 5\n",
    "n_layers = 3\n",
    "\n",
    "param_dict = {}\n",
    "param_dict['n_units']     = n_units\n",
    "param_dict['in_ch']       = 3\n",
    "param_dict['out_class']   = 100\n",
    "param_dict['width_list']  = [[256]*n_layers for _ in range(n_units)]\n",
    "param_dict['kernel_list'] = [[3]*n_layers for _ in range(n_units)]\n",
    "param_dict['bias_list']   = [[True]*n_layers for _ in range(n_units)]\n",
    "param_dict['bn']          = True\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 78,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Files already downloaded and verified\n",
      "Files already downloaded and verified\n",
      "Epoch 1\n",
      "\tTraining loss:3.21022367477417\n",
      "\tTraining time:28.91 s - 0.48 mins \n",
      "\tAccuracy of the mofa on the test images: 19 %\n",
      "Epoch 2\n",
      "\tTraining loss:2.6007723808288574\n",
      "\tTraining time:29.35 s - 0.49 mins \n",
      "\tAccuracy of the mofa on the test images: 30 %\n",
      "Epoch 3\n",
      "\tTraining loss:2.3514862060546875\n",
      "\tTraining time:29.56 s - 0.49 mins \n",
      "\tAccuracy of the mofa on the test images: 33 %\n",
      "Epoch 4\n",
      "\tTraining loss:1.8340553045272827\n",
      "\tTraining time:29.65 s - 0.49 mins \n",
      "\tAccuracy of the mofa on the test images: 44 %\n",
      "Epoch 5\n",
      "\tTraining loss:1.9492859840393066\n",
      "\tTraining time:29.69 s - 0.49 mins \n",
      "\tAccuracy of the mofa on the test images: 49 %\n",
      "Epoch 6\n",
      "\tTraining loss:1.6834275722503662\n",
      "\tTraining time:29.71 s - 0.50 mins \n",
      "\tAccuracy of the mofa on the test images: 53 %\n",
      "Epoch 7\n",
      "\tTraining loss:1.5695773363113403\n",
      "\tTraining time:29.73 s - 0.50 mins \n",
      "\tAccuracy of the mofa on the test images: 54 %\n",
      "Epoch 8\n",
      "\tTraining loss:1.3024191856384277\n",
      "\tTraining time:29.74 s - 0.50 mins \n",
      "\tAccuracy of the mofa on the test images: 56 %\n",
      "Epoch 9\n",
      "\tTraining loss:1.2754276990890503\n",
      "\tTraining time:29.75 s - 0.50 mins \n",
      "\tAccuracy of the mofa on the test images: 58 %\n",
      "Epoch 10\n",
      "\tTraining loss:1.2020167112350464\n",
      "\tTraining time:29.76 s - 0.50 mins \n",
      "\tAccuracy of the mofa on the test images: 57 %\n",
      "Epoch 11\n",
      "\tTraining loss:1.2194132804870605\n",
      "\tTraining time:29.76 s - 0.50 mins \n",
      "\tAccuracy of the mofa on the test images: 60 %\n",
      "Epoch 12\n",
      "\tTraining loss:1.2002625465393066\n",
      "\tTraining time:29.76 s - 0.50 mins \n",
      "\tAccuracy of the mofa on the test images: 60 %\n",
      "Epoch 13\n",
      "\tTraining loss:0.9628493785858154\n",
      "\tTraining time:29.76 s - 0.50 mins \n",
      "\tAccuracy of the mofa on the test images: 60 %\n",
      "Epoch 14\n",
      "\tTraining loss:0.749707043170929\n",
      "\tTraining time:29.75 s - 0.50 mins \n",
      "\tAccuracy of the mofa on the test images: 60 %\n",
      "Epoch 15\n",
      "\tTraining loss:0.8995128870010376\n",
      "\tTraining time:29.76 s - 0.50 mins \n",
      "\tAccuracy of the mofa on the test images: 62 %\n",
      "Epoch 16\n",
      "\tTraining loss:0.6700180768966675\n",
      "\tTraining time:29.76 s - 0.50 mins \n",
      "\tAccuracy of the mofa on the test images: 60 %\n",
      "Epoch 17\n",
      "\tTraining loss:0.7941420078277588\n",
      "\tTraining time:29.76 s - 0.50 mins \n",
      "\tAccuracy of the mofa on the test images: 62 %\n",
      "Epoch 18\n",
      "\tTraining loss:0.858849048614502\n",
      "\tTraining time:29.76 s - 0.50 mins \n",
      "\tAccuracy of the mofa on the test images: 64 %\n",
      "Epoch 19\n",
      "\tTraining loss:0.658119797706604\n",
      "\tTraining time:29.75 s - 0.50 mins \n",
      "\tAccuracy of the mofa on the test images: 63 %\n",
      "Epoch 20\n",
      "\tTraining loss:0.6623033285140991\n",
      "\tTraining time:29.76 s - 0.50 mins \n",
      "\tAccuracy of the mofa on the test images: 64 %\n",
      "Epoch 21\n",
      "\tTraining loss:0.6580643653869629\n",
      "\tTraining time:29.75 s - 0.50 mins \n",
      "\tAccuracy of the mofa on the test images: 64 %\n",
      "Epoch 22\n",
      "\tTraining loss:0.5349746942520142\n",
      "\tTraining time:29.75 s - 0.50 mins \n",
      "\tAccuracy of the mofa on the test images: 65 %\n",
      "Epoch 23\n",
      "\tTraining loss:0.5484212040901184\n",
      "\tTraining time:29.75 s - 0.50 mins \n",
      "\tAccuracy of the mofa on the test images: 66 %\n",
      "Epoch 24\n",
      "\tTraining loss:0.5829234719276428\n",
      "\tTraining time:29.75 s - 0.50 mins \n",
      "\tAccuracy of the mofa on the test images: 65 %\n",
      "Epoch 25\n",
      "\tTraining loss:0.5717816948890686\n",
      "\tTraining time:29.75 s - 0.50 mins \n",
      "\tAccuracy of the mofa on the test images: 64 %\n",
      "Epoch 26\n",
      "\tTraining loss:0.5034673810005188\n",
      "\tTraining time:29.75 s - 0.50 mins \n",
      "\tAccuracy of the mofa on the test images: 64 %\n",
      "Epoch 27\n",
      "\tTraining loss:0.41094598174095154\n",
      "\tTraining time:29.75 s - 0.50 mins \n",
      "\tAccuracy of the mofa on the test images: 64 %\n",
      "Epoch 28\n",
      "\tTraining loss:0.23923037946224213\n",
      "\tTraining time:29.75 s - 0.50 mins \n",
      "\tAccuracy of the mofa on the test images: 66 %\n",
      "Epoch 29\n",
      "\tTraining loss:0.3984849452972412\n",
      "\tTraining time:29.76 s - 0.50 mins \n",
      "\tAccuracy of the mofa on the test images: 65 %\n",
      "Epoch 30\n",
      "\tTraining loss:0.25974902510643005\n",
      "\tTraining time:29.76 s - 0.50 mins \n",
      "\tAccuracy of the mofa on the test images: 66 %\n",
      "Epoch 31\n",
      "\tTraining loss:0.23639890551567078\n",
      "\tTraining time:29.76 s - 0.50 mins \n",
      "\tAccuracy of the mofa on the test images: 66 %\n",
      "Epoch 32\n",
      "\tTraining loss:0.3411833941936493\n",
      "\tTraining time:29.75 s - 0.50 mins \n",
      "\tAccuracy of the mofa on the test images: 66 %\n",
      "Epoch 33\n",
      "\tTraining loss:0.292403906583786\n",
      "\tTraining time:29.76 s - 0.50 mins \n",
      "\tAccuracy of the mofa on the test images: 66 %\n",
      "Epoch 34\n",
      "\tTraining loss:0.31061887741088867\n",
      "\tTraining time:29.75 s - 0.50 mins \n",
      "\tAccuracy of the mofa on the test images: 66 %\n",
      "Epoch 35\n",
      "\tTraining loss:0.22688177227973938\n",
      "\tTraining time:29.76 s - 0.50 mins \n",
      "\tAccuracy of the mofa on the test images: 64 %\n",
      "Epoch 36\n",
      "\tTraining loss:0.1167796328663826\n",
      "\tTraining time:29.75 s - 0.50 mins \n",
      "\tAccuracy of the mofa on the test images: 70 %\n",
      "Epoch 37\n",
      "\tTraining loss:0.059219107031822205\n",
      "\tTraining time:29.76 s - 0.50 mins \n",
      "\tAccuracy of the mofa on the test images: 70 %\n",
      "Epoch 38\n",
      "\tTraining loss:0.06147979572415352\n",
      "\tTraining time:29.76 s - 0.50 mins \n",
      "\tAccuracy of the mofa on the test images: 70 %\n",
      "Epoch 39\n",
      "\tTraining loss:0.07753048837184906\n",
      "\tTraining time:29.76 s - 0.50 mins \n",
      "\tAccuracy of the mofa on the test images: 69 %\n",
      "Epoch 40\n",
      "\tTraining loss:0.0658850446343422\n",
      "\tTraining time:29.75 s - 0.50 mins \n",
      "\tAccuracy of the mofa on the test images: 69 %\n",
      "Epoch 41\n",
      "\tTraining loss:0.09041500836610794\n",
      "\tTraining time:29.76 s - 0.50 mins \n",
      "\tAccuracy of the mofa on the test images: 69 %\n",
      "Epoch 42\n",
      "\tTraining loss:0.051054760813713074\n",
      "\tTraining time:29.76 s - 0.50 mins \n",
      "\tAccuracy of the mofa on the test images: 69 %\n",
      "Epoch 43\n",
      "\tTraining loss:0.10945221781730652\n",
      "\tTraining time:29.76 s - 0.50 mins \n",
      "\tAccuracy of the mofa on the test images: 69 %\n",
      "Epoch 44\n",
      "\tTraining loss:0.06263703107833862\n",
      "\tTraining time:29.76 s - 0.50 mins \n",
      "\tAccuracy of the mofa on the test images: 69 %\n",
      "Epoch 45\n",
      "\tTraining loss:0.04292244836688042\n",
      "\tTraining time:29.76 s - 0.50 mins \n",
      "\tAccuracy of the mofa on the test images: 69 %\n",
      "Epoch 46\n",
      "\tTraining loss:0.1660776138305664\n",
      "\tTraining time:29.76 s - 0.50 mins \n",
      "\tAccuracy of the mofa on the test images: 69 %\n",
      "Epoch 47\n",
      "\tTraining loss:0.048292480409145355\n",
      "\tTraining time:29.76 s - 0.50 mins \n",
      "\tAccuracy of the mofa on the test images: 69 %\n",
      "Epoch 48\n",
      "\tTraining loss:0.09133859723806381\n",
      "\tTraining time:29.76 s - 0.50 mins \n",
      "\tAccuracy of the mofa on the test images: 69 %\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-78-fdd58572648c>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m     47\u001b[0m     \u001b[0mmofa\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtrain\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     48\u001b[0m     \u001b[0;32mfor\u001b[0m \u001b[0mbatch\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mlabels\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mtrainset\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 49\u001b[0;31m         \u001b[0mbatch\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mlabels\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mbatch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mto\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdevice\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mlabels\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mto\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdevice\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     50\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     51\u001b[0m         \u001b[0my_pred\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mmofa\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mbatch\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "use_cuda = torch.cuda.is_available()\n",
    "device = torch.device(\"cuda:0\" if use_cuda else \"cpu\")\n",
    "\n",
    "class Args():\n",
    "    def __init__(self):\n",
    "        super(Args, self).__init__()\n",
    "        self.truncate_testset = False\n",
    "        self.act_mode_8bit = False\n",
    "        \n",
    "args = Args()\n",
    "train_dataset, test_dataset = cifar100_get_datasets(('data', args))\n",
    "\n",
    "trainset = DataLoader(dataset=train_dataset,\n",
    "                      batch_size=100,\n",
    "                      shuffle=True,\n",
    "                      num_workers=0)\n",
    "\n",
    "valset = DataLoader(dataset=test_dataset,\n",
    "                      batch_size=1000,\n",
    "                      shuffle=False,\n",
    "                      num_workers=0)\n",
    "\n",
    "mofa = MOFAnet(param_dict)\n",
    "mofa = mofa.to(device)\n",
    "\n",
    "criterion = torch.nn.CrossEntropyLoss()\n",
    "optimizer = torch.optim.Adam(mofa.parameters(), lr=1e-4)\n",
    "scheduler = StepLR(optimizer, step_size=35, gamma=0.4)\n",
    "\n",
    "best_val_accuracy = 0\n",
    "max_epochs = 150\n",
    "for epoch in range(max_epochs):\n",
    "    t0 = time.time()\n",
    "    mofa.train()\n",
    "    for batch, labels in trainset:\n",
    "        batch, labels = batch.to(device), labels.to(device)\n",
    "        \n",
    "        y_pred = mofa(batch)\n",
    "        loss = criterion(y_pred, labels)\n",
    "        \n",
    "        optimizer.zero_grad()\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "    scheduler.step()\n",
    "    \n",
    "    print(f'Epoch {epoch+1}')\n",
    "    print(f'\\tTraining loss:{loss.item()}')\n",
    "    t1 = time.time()\n",
    "    print(f'\\tTraining time:{t1-t0:.2f} s - {(t1-t0)/60:.2f} mins ')\n",
    "    # Validation\n",
    "    correct = 0\n",
    "    total = 0\n",
    "    mofa.eval()\n",
    "    with torch.no_grad():\n",
    "        for data in valset:\n",
    "            images, labels = data\n",
    "            images, labels = images.to(device), labels.to(device)\n",
    "            outputs = mofa(images)\n",
    "            _, predicted = torch.max(outputs.data, 1)\n",
    "            total += labels.size(0)\n",
    "            correct += (predicted == labels).sum().item()\n",
    "    val_accuracy = correct / total\n",
    "    if val_accuracy > best_val_accuracy:\n",
    "        if epoch != 0:\n",
    "            os.remove(f'mofa_models/noclamp_mofa_acc{100*best_val_accuracy:.0f}%.pth.tar')\n",
    "        torch.save(mofa, f'mofa_models/noclamp_mofa_acc{100*val_accuracy:.0f}%.pth.tar')\n",
    "        best_val_accuracy = val_accuracy\n",
    "    print('\\tAccuracy of the mofa on the test images: %d %%' % (\n",
    "        100 * correct / total))\n",
    "    "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# <ins> Elastic Kernel Training </ins>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = torch.load('mofa_models/noclamp_mofa_acc71%.pth.tar')\n",
    "use_cuda = torch.cuda.is_available()\n",
    "device = torch.device(\"cuda:0\" if use_cuda else \"cpu\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "fused_model = fuse_bn_mofa(model)\n",
    "mofa = copy.deepcopy(fused_model)\n",
    "fused_model = fused_model.to(device)\n",
    "mofa = mofa.to(device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Files already downloaded and verified\n",
      "Files already downloaded and verified\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "<ipython-input-15-9ec76991f31d>:3: UserWarning: Implicit dimension choice for log_softmax has been deprecated. Change the call to include dim=X as an argument.\n",
      "  return torch.mean(torch.sum(- soft_target * logsoftmax(pred), 1))\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1\n",
      "\tTraining loss:6.849119663238525\n",
      "\tTraining time:58.44 s - 0.97 mins \n",
      "\tAccuracy of the mofa on the test images: 63 %\n",
      "\tFirst ktm: 1.0893168449401855\n",
      "\tLast ktm: 1.0\n",
      "Epoch 2\n",
      "\tTraining loss:1.3607887029647827\n",
      "\tTraining time:58.56 s - 0.98 mins \n",
      "\tAccuracy of the mofa on the test images: 65 %\n",
      "\tFirst ktm: 1.2011218070983887\n",
      "\tLast ktm: 1.0\n",
      "Epoch 3\n",
      "\tTraining loss:0.793714165687561\n",
      "\tTraining time:58.43 s - 0.97 mins \n",
      "\tAccuracy of the mofa on the test images: 66 %\n",
      "\tFirst ktm: 1.2770159244537354\n",
      "\tLast ktm: 1.0\n",
      "Epoch 4\n",
      "\tTraining loss:1.04897141456604\n",
      "\tTraining time:58.57 s - 0.98 mins \n",
      "\tAccuracy of the mofa on the test images: 66 %\n",
      "\tFirst ktm: 1.3380467891693115\n",
      "\tLast ktm: 1.0\n",
      "Epoch 5\n",
      "\tTraining loss:1.8602148294448853\n",
      "\tTraining time:58.25 s - 0.97 mins \n",
      "\tAccuracy of the mofa on the test images: 66 %\n",
      "\tFirst ktm: 1.3763775825500488\n",
      "\tLast ktm: 1.0\n",
      "Epoch 6\n",
      "\tTraining loss:1.184467077255249\n",
      "\tTraining time:58.26 s - 0.97 mins \n",
      "\tAccuracy of the mofa on the test images: 66 %\n",
      "\tFirst ktm: 1.4150092601776123\n",
      "\tLast ktm: 1.0\n",
      "Epoch 7\n",
      "\tTraining loss:0.3548888564109802\n",
      "\tTraining time:58.26 s - 0.97 mins \n",
      "\tAccuracy of the mofa on the test images: 68 %\n",
      "\tFirst ktm: 1.4432635307312012\n",
      "\tLast ktm: 1.0\n",
      "Epoch 8\n",
      "\tTraining loss:0.3969695270061493\n",
      "\tTraining time:58.63 s - 0.98 mins \n",
      "\tAccuracy of the mofa on the test images: 68 %\n",
      "\tFirst ktm: 1.462303876876831\n",
      "\tLast ktm: 1.0\n",
      "Epoch 9\n",
      "\tTraining loss:0.7019582390785217\n",
      "\tTraining time:60.02 s - 1.00 mins \n",
      "\tAccuracy of the mofa on the test images: 68 %\n",
      "\tFirst ktm: 1.484333872795105\n",
      "\tLast ktm: 1.0\n",
      "Epoch 10\n",
      "\tTraining loss:0.4174271523952484\n",
      "\tTraining time:58.81 s - 0.98 mins \n",
      "\tAccuracy of the mofa on the test images: 68 %\n",
      "\tFirst ktm: 1.5073856115341187\n",
      "\tLast ktm: 1.0\n",
      "Epoch 11\n",
      "\tTraining loss:0.24422556161880493\n",
      "\tTraining time:58.60 s - 0.98 mins \n",
      "\tAccuracy of the mofa on the test images: 68 %\n",
      "\tFirst ktm: 1.5245513916015625\n",
      "\tLast ktm: 1.0\n",
      "Epoch 12\n",
      "\tTraining loss:0.6997882127761841\n",
      "\tTraining time:58.96 s - 0.98 mins \n",
      "\tAccuracy of the mofa on the test images: 66 %\n",
      "\tFirst ktm: 1.5400185585021973\n",
      "\tLast ktm: 1.0\n",
      "Epoch 13\n",
      "\tTraining loss:0.21903884410858154\n",
      "\tTraining time:58.70 s - 0.98 mins \n",
      "\tAccuracy of the mofa on the test images: 68 %\n",
      "\tFirst ktm: 1.5553855895996094\n",
      "\tLast ktm: 1.0\n",
      "Epoch 14\n",
      "\tTraining loss:2.6449837684631348\n",
      "\tTraining time:58.97 s - 0.98 mins \n",
      "\tAccuracy of the mofa on the test images: 68 %\n",
      "\tFirst ktm: 1.5696378946304321\n",
      "\tLast ktm: 1.0\n",
      "Epoch 15\n",
      "\tTraining loss:3.139227867126465\n",
      "\tTraining time:58.74 s - 0.98 mins \n",
      "\tAccuracy of the mofa on the test images: 68 %\n",
      "\tFirst ktm: 1.583090901374817\n",
      "\tLast ktm: 1.0\n",
      "Epoch 16\n",
      "\tTraining loss:0.5409178733825684\n",
      "\tTraining time:59.00 s - 0.98 mins \n",
      "\tAccuracy of the mofa on the test images: 68 %\n",
      "\tFirst ktm: 1.5950325727462769\n",
      "\tLast ktm: 1.0\n",
      "Epoch 17\n",
      "\tTraining loss:2.0112392902374268\n",
      "\tTraining time:58.80 s - 0.98 mins \n",
      "\tAccuracy of the mofa on the test images: 67 %\n",
      "\tFirst ktm: 1.6068639755249023\n",
      "\tLast ktm: 1.0\n",
      "Epoch 18\n",
      "\tTraining loss:0.3147977590560913\n",
      "\tTraining time:59.37 s - 0.99 mins \n",
      "\tAccuracy of the mofa on the test images: 69 %\n",
      "\tFirst ktm: 1.6178112030029297\n",
      "\tLast ktm: 1.0\n",
      "Epoch 19\n",
      "\tTraining loss:0.17915958166122437\n",
      "\tTraining time:58.00 s - 0.97 mins \n",
      "\tAccuracy of the mofa on the test images: 69 %\n",
      "\tFirst ktm: 1.6253610849380493\n",
      "\tLast ktm: 1.0\n",
      "Epoch 20\n",
      "\tTraining loss:0.37624603509902954\n",
      "\tTraining time:58.60 s - 0.98 mins \n",
      "\tAccuracy of the mofa on the test images: 69 %\n",
      "\tFirst ktm: 1.6356253623962402\n",
      "\tLast ktm: 1.0\n",
      "Epoch 21\n",
      "\tTraining loss:0.40765899419784546\n",
      "\tTraining time:59.47 s - 0.99 mins \n",
      "\tAccuracy of the mofa on the test images: 69 %\n",
      "\tFirst ktm: 1.6441086530685425\n",
      "\tLast ktm: 1.0\n",
      "Epoch 22\n",
      "\tTraining loss:0.5516945719718933\n",
      "\tTraining time:58.67 s - 0.98 mins \n",
      "\tAccuracy of the mofa on the test images: 69 %\n",
      "\tFirst ktm: 1.654710054397583\n",
      "\tLast ktm: 1.0\n",
      "Epoch 23\n",
      "\tTraining loss:0.47992783784866333\n",
      "\tTraining time:58.49 s - 0.97 mins \n",
      "\tAccuracy of the mofa on the test images: 69 %\n",
      "\tFirst ktm: 1.6604567766189575\n",
      "\tLast ktm: 1.0\n",
      "Epoch 24\n",
      "\tTraining loss:0.42338573932647705\n",
      "\tTraining time:58.63 s - 0.98 mins \n",
      "\tAccuracy of the mofa on the test images: 69 %\n",
      "\tFirst ktm: 1.6679747104644775\n",
      "\tLast ktm: 1.0\n",
      "Epoch 25\n",
      "\tTraining loss:0.33724504709243774\n",
      "\tTraining time:58.64 s - 0.98 mins \n",
      "\tAccuracy of the mofa on the test images: 69 %\n",
      "\tFirst ktm: 1.6758087873458862\n",
      "\tLast ktm: 1.0\n",
      "Epoch 26\n",
      "\tTraining loss:0.14136075973510742\n",
      "\tTraining time:58.78 s - 0.98 mins \n",
      "\tAccuracy of the mofa on the test images: 69 %\n",
      "\tFirst ktm: 1.6810122728347778\n",
      "\tLast ktm: 1.0\n",
      "Epoch 27\n",
      "\tTraining loss:0.35114455223083496\n",
      "\tTraining time:58.90 s - 0.98 mins \n",
      "\tAccuracy of the mofa on the test images: 69 %\n",
      "\tFirst ktm: 1.6865482330322266\n",
      "\tLast ktm: 1.0\n",
      "Epoch 28\n",
      "\tTraining loss:0.3887299597263336\n",
      "\tTraining time:58.66 s - 0.98 mins \n",
      "\tAccuracy of the mofa on the test images: 69 %\n",
      "\tFirst ktm: 1.6934596300125122\n",
      "\tLast ktm: 1.0\n",
      "Epoch 29\n",
      "\tTraining loss:0.3133547008037567\n",
      "\tTraining time:58.28 s - 0.97 mins \n",
      "\tAccuracy of the mofa on the test images: 69 %\n",
      "\tFirst ktm: 1.7012041807174683\n",
      "\tLast ktm: 1.0\n",
      "Epoch 30\n",
      "\tTraining loss:0.24052344262599945\n",
      "\tTraining time:58.97 s - 0.98 mins \n",
      "\tAccuracy of the mofa on the test images: 69 %\n",
      "\tFirst ktm: 1.7078875303268433\n",
      "\tLast ktm: 1.0\n",
      "Epoch 31\n",
      "\tTraining loss:0.26178479194641113\n",
      "\tTraining time:58.28 s - 0.97 mins \n",
      "\tAccuracy of the mofa on the test images: 69 %\n",
      "\tFirst ktm: 1.7127337455749512\n",
      "\tLast ktm: 1.0\n",
      "Epoch 32\n",
      "\tTraining loss:0.1497897207736969\n",
      "\tTraining time:58.38 s - 0.97 mins \n",
      "\tAccuracy of the mofa on the test images: 69 %\n",
      "\tFirst ktm: 1.7161520719528198\n",
      "\tLast ktm: 1.0\n",
      "Epoch 33\n",
      "\tTraining loss:0.2508077025413513\n",
      "\tTraining time:58.05 s - 0.97 mins \n",
      "\tAccuracy of the mofa on the test images: 69 %\n",
      "\tFirst ktm: 1.722084879875183\n",
      "\tLast ktm: 1.0\n",
      "Epoch 34\n",
      "\tTraining loss:1.697280764579773\n",
      "\tTraining time:58.97 s - 0.98 mins \n",
      "\tAccuracy of the mofa on the test images: 69 %\n",
      "\tFirst ktm: 1.7277309894561768\n",
      "\tLast ktm: 1.0\n",
      "Epoch 35\n",
      "\tTraining loss:0.16112345457077026\n",
      "\tTraining time:59.45 s - 0.99 mins \n",
      "\tAccuracy of the mofa on the test images: 68 %\n",
      "\tFirst ktm: 1.7314423322677612\n",
      "\tLast ktm: 1.0\n",
      "Epoch 36\n",
      "\tTraining loss:1.5530041456222534\n",
      "\tTraining time:58.44 s - 0.97 mins \n",
      "\tAccuracy of the mofa on the test images: 68 %\n",
      "\tFirst ktm: 1.735643744468689\n",
      "\tLast ktm: 1.0\n",
      "Epoch 37\n",
      "\tTraining loss:0.1463640332221985\n",
      "\tTraining time:58.16 s - 0.97 mins \n",
      "\tAccuracy of the mofa on the test images: 70 %\n",
      "\tFirst ktm: 1.742524266242981\n",
      "\tLast ktm: 1.0\n",
      "Epoch 38\n",
      "\tTraining loss:0.18792769312858582\n",
      "\tTraining time:58.50 s - 0.98 mins \n",
      "\tAccuracy of the mofa on the test images: 69 %\n",
      "\tFirst ktm: 1.7454519271850586\n",
      "\tLast ktm: 1.0\n",
      "Epoch 39\n",
      "\tTraining loss:0.2584853768348694\n",
      "\tTraining time:58.72 s - 0.98 mins \n",
      "\tAccuracy of the mofa on the test images: 69 %\n",
      "\tFirst ktm: 1.7497655153274536\n",
      "\tLast ktm: 1.0\n",
      "Epoch 40\n",
      "\tTraining loss:0.16131344437599182\n",
      "\tTraining time:58.39 s - 0.97 mins \n",
      "\tAccuracy of the mofa on the test images: 70 %\n",
      "\tFirst ktm: 1.754621148109436\n",
      "\tLast ktm: 1.0\n",
      "Epoch 41\n",
      "\tTraining loss:0.23365074396133423\n",
      "\tTraining time:59.08 s - 0.98 mins \n",
      "\tAccuracy of the mofa on the test images: 70 %\n",
      "\tFirst ktm: 1.758124828338623\n",
      "\tLast ktm: 1.0\n",
      "Epoch 42\n",
      "\tTraining loss:0.4739166498184204\n",
      "\tTraining time:58.43 s - 0.97 mins \n",
      "\tAccuracy of the mofa on the test images: 69 %\n",
      "\tFirst ktm: 1.7620686292648315\n",
      "\tLast ktm: 1.0\n",
      "Epoch 43\n",
      "\tTraining loss:0.13910600543022156\n",
      "\tTraining time:58.74 s - 0.98 mins \n",
      "\tAccuracy of the mofa on the test images: 70 %\n",
      "\tFirst ktm: 1.7672090530395508\n",
      "\tLast ktm: 1.0\n",
      "Epoch 44\n",
      "\tTraining loss:0.1663839966058731\n",
      "\tTraining time:58.91 s - 0.98 mins \n",
      "\tAccuracy of the mofa on the test images: 69 %\n",
      "\tFirst ktm: 1.772016167640686\n",
      "\tLast ktm: 1.0\n",
      "Epoch 45\n",
      "\tTraining loss:0.19804701209068298\n",
      "\tTraining time:58.95 s - 0.98 mins \n",
      "\tAccuracy of the mofa on the test images: 70 %\n",
      "\tFirst ktm: 1.7755879163742065\n",
      "\tLast ktm: 1.0\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                      Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-16-06c39e68d342>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m     80\u001b[0m         \u001b[0moptimizer\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mstep\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     81\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 82\u001b[0;31m         \u001b[0mmofa\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mupdate_mofa_from_subnet_kernel\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mmofa\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0msubnet\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     83\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     84\u001b[0m     \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34mf'Epoch {epoch+1}'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m<ipython-input-13-c5fb7302ccdc>\u001b[0m in \u001b[0;36mupdate_mofa_from_subnet_kernel\u001b[0;34m(mofa, subnet)\u001b[0m\n\u001b[1;32m     26\u001b[0m         \u001b[0;32mfor\u001b[0m \u001b[0mu_ind\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0munit\u001b[0m \u001b[0;32min\u001b[0m \u001b[0menumerate\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mmofa\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0munits\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     27\u001b[0m             \u001b[0;32mfor\u001b[0m \u001b[0ml_ind\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mlayer\u001b[0m \u001b[0;32min\u001b[0m \u001b[0menumerate\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0munit\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mlayers\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 28\u001b[0;31m                 \u001b[0mmofa\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0munits\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mu_ind\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mlayers\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0ml_ind\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mconv2d\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mweight\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcopy_\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0msubnet\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0munits\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mu_ind\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mlayers\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0ml_ind\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mconv2d\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mweight\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     29\u001b[0m                 \u001b[0mmofa\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0munits\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mu_ind\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mlayers\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0ml_ind\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mktm\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcopy_\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0msubnet\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0munits\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mu_ind\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mlayers\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0ml_ind\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mktm\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     30\u001b[0m                 \u001b[0;32mif\u001b[0m \u001b[0mmofa\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mbias_list\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mu_ind\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0ml_ind\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0;32mTrue\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "kd_ratio = 0.5\n",
    "\n",
    "class Args():\n",
    "    def __init__(self):\n",
    "        super(Args, self).__init__()\n",
    "        self.truncate_testset = False\n",
    "        self.act_mode_8bit = False\n",
    "        \n",
    "args = Args()\n",
    "train_dataset, test_dataset = cifar100_get_datasets(('data', args))\n",
    "\n",
    "trainset = DataLoader(dataset=train_dataset,\n",
    "                      batch_size=100,\n",
    "                      shuffle=True,\n",
    "                      num_workers=0)\n",
    "\n",
    "valset = DataLoader(dataset=test_dataset,\n",
    "                      batch_size=1000,\n",
    "                      shuffle=False,\n",
    "                      num_workers=0)\n",
    "\n",
    "criterion = torch.nn.CrossEntropyLoss()\n",
    "\n",
    "best_val_accuracy = 0\n",
    "max_epochs = 250\n",
    "for epoch in range(max_epochs):\n",
    "    t0 = time.time()\n",
    "    mofa.train()\n",
    "    for batch, labels in trainset:\n",
    "        batch, labels = batch.to(device), labels.to(device)\n",
    "        \n",
    "#         mofa = make_bn_stats_false(mofa)\n",
    "        subnet = sample_subnet_kernel(mofa)\n",
    "        subnet = subnet.to(device)\n",
    "        optimizer = torch.optim.SGD(subnet.parameters(), lr=1e-3)\n",
    "      \n",
    "        y_pred = subnet(batch)\n",
    "        \n",
    "        if kd_ratio > 0:\n",
    "            fused_model.train()\n",
    "            with torch.no_grad():\n",
    "                soft_logits = fused_model(batch).detach()\n",
    "                soft_label = F.softmax(soft_logits, dim=1)\n",
    "            kd_loss = cross_entropy_loss_with_soft_target(y_pred, soft_label)\n",
    "            loss = kd_ratio * kd_loss + criterion(y_pred, labels)\n",
    "        else:\n",
    "            loss = criterion(y_pred, labels)     \n",
    "        \n",
    "        optimizer.zero_grad()\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "        \n",
    "        mofa = update_mofa_from_subnet_kernel(mofa, subnet)\n",
    "        \n",
    "    print(f'Epoch {epoch+1}')\n",
    "    print(f'\\tTraining loss:{loss.item()}')\n",
    "    t1 = time.time()\n",
    "    print(f'\\tTraining time:{t1-t0:.2f} s - {(t1-t0)/60:.2f} mins ')\n",
    "    \n",
    "    # Validation\n",
    "    correct = 0\n",
    "    total = 0\n",
    "    mofa.eval()\n",
    "    with torch.no_grad():\n",
    "#         mofa = make_bn_stats_true(mofa)\n",
    "#         mofa.train()\n",
    "#         for data in valset:\n",
    "#             images, labels = data\n",
    "#             images, labels = images.to(device), labels.to(device)\n",
    "#             outputs = mofa(images)\n",
    "#         mofa.eval()\n",
    "        for data in valset:\n",
    "            images, labels = data\n",
    "            images, labels = images.to(device), labels.to(device)\n",
    "            outputs = mofa(images)\n",
    "            _, predicted = torch.max(outputs.data, 1)\n",
    "            total += labels.size(0)\n",
    "            correct += (predicted == labels).sum().item()\n",
    "    val_accuracy = correct / total\n",
    "#     if val_accuracy > best_val_accuracy:\n",
    "#         if epoch is not 0:\n",
    "#             os.remove(f'mofa_models/ofa_acc{100*best_val_accuracy:.0f}%.pth.tar')\n",
    "#         torch.save(mofa, f'mofa_models/ofa_acc{100*val_accuracy:.0f}%.pth.tar')\n",
    "#         best_val_accuracy = val_accuracy\n",
    "    print('\\tAccuracy of the mofa on the test images: %d %%' % (\n",
    "        100 * correct / total))\n",
    "    print(f'\\tFirst ktm: {mofa.units[0].layers[0].ktm[4].item()}')\n",
    "    print(f'\\tLast ktm: {mofa.units[4].layers[2].ktm[4].item()}')\n",
    "    "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# <ins> Testing <ins/>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**MOFA Test**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 97,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.6512\n"
     ]
    }
   ],
   "source": [
    "correct = 0\n",
    "total = 0\n",
    "mofa.eval()\n",
    "with torch.no_grad():\n",
    "    for data in valset:\n",
    "        images, labels = data\n",
    "        images, labels = images.to(device), labels.to(device)\n",
    "        outputs = mofa(images)\n",
    "        _, predicted = torch.max(outputs.data, 1)\n",
    "        total += labels.size(0)\n",
    "        correct += (predicted == labels).sum().item()\n",
    "print(correct / total)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Random Subnet Test**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {},
   "outputs": [],
   "source": [
    "subnet = sample_subnet_kernel(mofa)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'n_units': 5,\n",
       " 'in_ch': 3,\n",
       " 'out_class': 100,\n",
       " 'width_list': [[256, 256, 256],\n",
       "  [256, 256, 256],\n",
       "  [256, 256, 256],\n",
       "  [256, 256, 256],\n",
       "  [256, 256, 256]],\n",
       " 'kernel_list': [[1, 3, 1], [3, 3, 3], [3, 3, 3], [3, 3, 3], [3, 3, 3]],\n",
       " 'bias_list': [[True, True, True],\n",
       "  [True, True, True],\n",
       "  [True, True, True],\n",
       "  [True, True, True],\n",
       "  [True, True, True]],\n",
       " 'bn': False}"
      ]
     },
     "execution_count": 45,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "print(subnet.param_dict)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.674\n"
     ]
    }
   ],
   "source": [
    "correct = 0\n",
    "total = 0\n",
    "subnet = subnet.to(device)\n",
    "subnet.eval()\n",
    "with torch.no_grad():\n",
    "    for data in valset:\n",
    "        images, labels = data\n",
    "        images, labels = images.to(device), labels.to(device)\n",
    "        outputs = subnet(images)\n",
    "        _, predicted = torch.max(outputs.data, 1)\n",
    "        total += labels.size(0)\n",
    "        correct += (predicted == labels).sum().item()\n",
    "        break\n",
    "#     val_accuracy = correct / total\n",
    "#     if val_accuracy > best_val_accuracy:\n",
    "#         if epoch is not 0:\n",
    "#             os.remove(f'mofa_models/ofa_acc{100*best_val_accuracy:.0f}%.pth.tar')\n",
    "#         torch.save(mofa, f'mofa_models/ofa_acc{100*val_accuracy:.0f}%.pth.tar')\n",
    "#         best_val_accuracy = val_accuracy\n",
    "print(correct / total)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
