lr_schedulers:
  training_lr:
    class: MultiStepLR
    milestones: [60, 120, 240, 480]
    gamma: 0.5

quantizers:
  linear_quantizer:
    class: QuantAwareTrainRangeLinearQuantizer
    bits_activations: 8
    bits_weights: 2
    mode: 'SYMMETRIC'  # Can try "SYMMETRIC" as well
    ema_decay: 0.999
    per_channel_wts: False
    
    
policies:
    - lr_scheduler:
        instance_name: training_lr
      starting_epoch: 0
      ending_epoch: 600
      frequency: 1

    - quantizer:
        instance_name: linear_quantizer
      # For now putting a large range here, which should cover both training from scratch or resuming from some
      # pre-trained checkpoint at some unknown epoch
      starting_epoch: 0
      ending_epoch: 600
      frequency: 1
